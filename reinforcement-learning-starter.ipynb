{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning with OpenAI Gym\n",
    "---\n",
    "This notebook will create and test different reinforcement learning agents and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Environment\n",
    "---\n",
    "Call `gym.make(\"environment name\")` to load a new environment.\n",
    "\n",
    "Check out the list of available environments at <https://gym.openai.com/envs/>\n",
    "\n",
    "Edit this cell to load different environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load an environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# If err that env doesn't exist, run ' pip install 'gym[all]' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print observation and action spaces\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an Agent\n",
    "---\n",
    "\n",
    "Reset the environment before each run with `env.reset`\n",
    "\n",
    "Step forward through the environment to get new observations and rewards over time with `env.step`\n",
    "\n",
    "`env.step` takes a parameter for the action to take on this step and returns the following:\n",
    "- Observations for this step\n",
    "- Rewards earned this step\n",
    "- \"Done\", a boolean value indicating if the game is finished\n",
    "- Info - some debug information that some environments provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n",
      "14.0\n",
      "12.0\n",
      "35.0\n",
      "28.0\n",
      "18.0\n",
      "28.0\n",
      "21.0\n",
      "11.0\n",
      "35.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make a random agent\n",
    "games_to_play = 10\n",
    "\n",
    "for i in range(games_to_play):\n",
    "    # Reset the env\n",
    "    obs = env.reset()  # initialize all vars and prep game to run\n",
    "    episode_rewards = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()  # draws frame of the game\n",
    "        \n",
    "        action = env.action_space.sample()  # choose action randomly\n",
    "        \n",
    "        # Take a step in the env with the chosen action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        \n",
    "    print(episode_rewards)  # print total rewards when done\n",
    "\n",
    "\n",
    "env.close()  # close the env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradients\n",
    "---\n",
    "The policy gradients algorithm records gameplay over a training period, then runs the results of the actions chosen through a neural network, making successful actions that resulted in a reward more likely, and unsuccessful actions less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build the policy gradient neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounting and Normalizing Rewards\n",
    "---\n",
    "In order to determine how \"successful\" a given action is, the policy gradient algorithm evaluates each action based on how many rewards were earned after it was performed in an episode.\n",
    "\n",
    "The discount rewards function goes through each time step of an episode and tracks the total rewards earned from each step to the end of the episode.\n",
    "\n",
    "For example, if an episode took 10 steps to finish, and the agent earns 1 point of reward every step, the rewards for each frame would be stored as \n",
    "`[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]`\n",
    "\n",
    "This allows the agent to credit early actions that didn't lose the game with future success, and later actions (that likely resulted in the end of the game) to get less credit.\n",
    "\n",
    "One disadvantage of arranging rewards like this is that early actions didn't necessarily directly contribute to later rewards, so a **discount factor** is applied that scales rewards down over time. A discount factor < 1 means that rewards earned closer to the current time step will be worth more than rewards earned later.\n",
    "\n",
    "With our reward example above, if we applied a discount factor of .90, the rewards would be stored as\n",
    "`[ 6.5132156   6.12579511  5.6953279   5.217031    4.68559     4.0951      3.439\n",
    "  2.71        1.9         1. ]`\n",
    "\n",
    "This means that the early actions still get more credit than later actions, but not the full value of the rewards for the entire episode.\n",
    "\n",
    "Finally, the rewards are normalized to lower the variance between reward values in longer or shorter episodes.\n",
    "\n",
    "You can tweak the discount factor as one of the hyperparameters of your model to find one that fits your task the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the discounted and normalized rewards function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure\n",
    "---\n",
    "The agent will play games and record the history of the episode. At the end of every game, the episode's history will be processed to calculate the **gradients** that the model learned from that episode.\n",
    "\n",
    "Every few games the calculated gradients will be applied, updating the model's parameters with the lessons from the games so far.\n",
    "\n",
    "While training, you'll keep track of average scores and render the environment occasionally to see your model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "---\n",
    "\n",
    "This cell will run through games choosing actions without the learning process so you can see how your model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
