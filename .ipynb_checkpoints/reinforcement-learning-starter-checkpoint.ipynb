{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning with OpenAI Gym\n",
    "---\n",
    "This notebook will create and test different reinforcement learning agents and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Environment\n",
    "---\n",
    "Call `gym.make(\"environment name\")` to load a new environment.\n",
    "\n",
    "Check out the list of available environments at <https://gym.openai.com/envs/>\n",
    "\n",
    "Edit this cell to load different environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idstudent/.conda/envs/tensorflow-env/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load an environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# If err that env doesn't exist, run ' pip install 'gym[all]' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print observation and action spaces\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an Agent\n",
    "---\n",
    "\n",
    "Reset the environment before each run with `env.reset`\n",
    "\n",
    "Step forward through the environment to get new observations and rewards over time with `env.step`\n",
    "\n",
    "`env.step` takes a parameter for the action to take on this step and returns the following:\n",
    "- Observations for this step\n",
    "- Rewards earned this step\n",
    "- \"Done\", a boolean value indicating if the game is finished\n",
    "- Info - some debug information that some environments provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "20.0\n",
      "20.0\n",
      "23.0\n",
      "15.0\n",
      "23.0\n",
      "15.0\n",
      "22.0\n",
      "19.0\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make a random agent\n",
    "games_to_play = 10\n",
    "\n",
    "for i in range(games_to_play):\n",
    "    # Reset the env\n",
    "    obs = env.reset()  # initialize all vars and prep game to run\n",
    "    episode_rewards = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()  # draws frame of the game\n",
    "        \n",
    "        action = env.action_space.sample()  # choose action randomly\n",
    "        \n",
    "        # Take a step in the env with the chosen action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        \n",
    "    print(episode_rewards)  # print total rewards when done\n",
    "\n",
    "\n",
    "env.close()  # close the env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradients\n",
    "---\n",
    "The policy gradients algorithm records gameplay over a training period, then runs the results of the actions chosen through a neural network, making successful actions that resulted in a reward more likely, and unsuccessful actions less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build the policy gradient neural network\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, num_actions, state_size):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()  # initializes some starting values for the neurons\n",
    "        \n",
    "        # this will let someone pass any number of states into the network in a batch\n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, state_size])\n",
    "        \n",
    "        # Neural net starts here...\n",
    "        \n",
    "        # creates a hidden layer connected to input layer with 8 units, relu activation, and the xavier initializer\n",
    "        hidden_layer_1 = tf.layers.dense(self.input_layer, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_2 = tf.layers.dense(hidden_layer_1, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        \n",
    "        # Output of neural net...\n",
    "        \n",
    "        out = tf.layers.dense(hidden_layer_2, num_actions, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.softmax(out)\n",
    "        self.choice = tf.argmax(self.outputs, axis=1)  # ' axis=1 ' indicates maximum value of axis 1(action weights) is wanted\n",
    "        \n",
    "        self.rewards = tf.placeholder(shape=[None, ], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        one_hot_actions = tf.one_hot(self.actions, num_actions)\n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=one_hot_actions)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(cross_entropy * self.rewards)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tf.trainable_variables())\n",
    "        \n",
    "        # Create a placeholder list for gradients\n",
    "        self.gradients_to_apply = []\n",
    "        for index, variable in enumerate(tf.trainable_variables()):\n",
    "            gradient_placeholder = tf.placeholder(tf.float32)\n",
    "            self.gradients_to_apply.append(gradient_placeholder)\n",
    "            \n",
    "        # Create the operation to update gradients with the gradients placeholder...\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "        # Update gradients operation applies gradients that were fed into the corresponding trainable vars in the model\n",
    "            # Operation runs every time model needs to appy what it has learned from its games and update its parameters\n",
    "        self.update_gradients = optimizer.apply_gradients(zip(self.gradients_to_apply, tf.trainable_variables()))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounting and Normalizing Rewards\n",
    "---\n",
    "In order to determine how \"successful\" a given action is, the policy gradient algorithm evaluates each action based on how many rewards were earned after it was performed in an episode.\n",
    "\n",
    "The discount rewards function goes through each time step of an episode and tracks the total rewards earned from each step to the end of the episode.\n",
    "\n",
    "For example, if an episode took 10 steps to finish, and the agent earns 1 point of reward every step, the rewards for each frame would be stored as \n",
    "`[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]`\n",
    "\n",
    "This allows the agent to credit early actions that didn't lose the game with future success, and later actions (that likely resulted in the end of the game) to get less credit.\n",
    "\n",
    "One disadvantage of arranging rewards like this is that early actions didn't necessarily directly contribute to later rewards, so a **discount factor** is applied that scales rewards down over time. A discount factor < 1 means that rewards earned closer to the current time step will be worth more than rewards earned later.\n",
    "\n",
    "With our reward example above, if we applied a discount factor of .90, the rewards would be stored as\n",
    "`[ 6.5132156   6.12579511  5.6953279   5.217031    4.68559     4.0951      3.439\n",
    "  2.71        1.9         1. ]`\n",
    "\n",
    "This means that the early actions still get more credit than later actions, but not the full value of the rewards for the entire episode.\n",
    "\n",
    "Finally, the rewards are normalized to lower the variance between reward values in longer or shorter episodes.\n",
    "\n",
    "You can tweak the discount factor as one of the hyperparameters of your model to find one that fits your task the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the discounted and normalized rewards function\n",
    "discount_rate = 0.95\n",
    "\n",
    "\n",
    "def discount_normalize_rewards(rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        total_rewards = total_rewards * discount_rate + rewards[i]\n",
    "        discounted_rewards[i] = total_rewards\n",
    "        \n",
    "    # Normalize rewards across multiple game lengths\n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= np.std(discounted_rewards)\n",
    "    \n",
    "    return discounted_rewards\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure\n",
    "---\n",
    "The agent will play games and record the history of the episode. At the end of every game, the episode's history will be processed to calculate the **gradients** that the model learned from that episode.\n",
    "\n",
    "Every few games the calculated gradients will be applied, updating the model's parameters with the lessons from the games so far.\n",
    "\n",
    "While training, you'll keep track of average scores and render the environment occasionally to see your model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-78ca1e3685b5>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Average reward / 100 eps: 33.0\n",
      "Average reward / 100 eps: 21.04\n",
      "Average reward / 100 eps: 35.06\n",
      "Average reward / 100 eps: 114.82\n",
      "Average reward / 100 eps: 276.01\n",
      "Average reward / 100 eps: 207.42\n",
      "Average reward / 100 eps: 207.11\n",
      "Average reward / 100 eps: 205.12\n",
      "Average reward / 100 eps: 293.26\n",
      "Average reward / 100 eps: 498.4\n"
     ]
    }
   ],
   "source": [
    "# TODO Create the training loop\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Modify to match shape of actions and states in the env\n",
    "num_actions = 2\n",
    "state_size = 4\n",
    "\n",
    "path = \"./cartpole-pg/\"  # for checkpoints\n",
    "\n",
    "training_episodes = 1000\n",
    "max_steps_per_episode = 10000\n",
    "episode_batch_size = 5\n",
    "\n",
    "agent = Agent(num_actions, state_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_episode_rewards = []\n",
    "    \n",
    "    gradient_buffer = sess.run(tf.trainable_variables())\n",
    "    \n",
    "    for index, gradient in enumerate(gradient_buffer):\n",
    "        gradient_buffer[index] = gradient * 0\n",
    "    \n",
    "    for episode in range(training_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_history = []\n",
    "        episode_rewards = 0\n",
    "    \n",
    "        for step in range(max_steps_per_episode):\n",
    "            if episode % 100 == 0:\n",
    "                env.render()\n",
    "\n",
    "            # Gets weights for each action\n",
    "            action_probabilities = sess.run(agent.outputs, feed_dict={agent.input_layer: [state]})\n",
    "            action_choice = np.random.choice(range(num_actions), p=action_probabilities[0])\n",
    "\n",
    "            # Save the resulting states, rewards and whether the episode finished\n",
    "            state_next, reward, done, _ = env.step(action_choice)\n",
    "\n",
    "            episode_history.append([state, action_choice, reward, state_next])\n",
    "            state = state_next\n",
    "\n",
    "            episode_rewards += reward\n",
    "\n",
    "            if done or step + 1 == max_steps_per_episode:\n",
    "                total_episode_rewards.append(episode_rewards)\n",
    "                episode_history = np.array(episode_history)\n",
    "                # normalize rewards fn on the stored rewards in episode history\n",
    "                episode_history[:,2] = discount_normalize_rewards(episode_history[:,2])\n",
    "\n",
    "                ep_gradients = sess.run(agent.gradients, \n",
    "                                        feed_dict={agent.input_layer: np.vstack(episode_history[:,0]),\n",
    "                                                   agent.actions: episode_history[:,1],\n",
    "                                                   agent.rewards: episode_history[:,2]})\n",
    "\n",
    "                # add the gradients \n",
    "                for index, gradient in enumerate(ep_gradients):\n",
    "                    gradient_buffer[index] += gradient\n",
    "\n",
    "                break\n",
    "        if episode % episode_batch_size == 0:\n",
    "            feed_dict_gradients = dict(zip(agent.gradients_to_apply, gradient_buffer))\n",
    "            \n",
    "            sess.run(agent.update_gradients, feed_dict=feed_dict_gradients)\n",
    "            \n",
    "            for index, gradient in enumerate(gradient_buffer):\n",
    "                gradient_buffer[index] = gradient * 0\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                saver.save(sess, path + \"pg-checkpoint\", episode)\n",
    "                \n",
    "                print(\"Average reward / 100 eps: \" + str(np.mean(total_episode_rewards[-100:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "---\n",
    "\n",
    "This cell will run through games choosing actions without the learning process so you can see how your model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cartpole-pg/pg-checkpoint-900\n",
      "Rewards for episode 0: 500.0\n",
      "Rewards for episode 1: 500.0\n",
      "Rewards for episode 2: 500.0\n",
      "Rewards for episode 3: 500.0\n",
      "Rewards for episode 4: 500.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Create the testing loop\n",
    "testing_episodes = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "    \n",
    "    for episode in range(testing_episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_rewards = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            env.render()\n",
    "            \n",
    "            # Get Action\n",
    "            action_argmax = sess.run(agent.choice, feed_dict={agent.input_layer:[state]})\n",
    "            action_choice = action_argmax[0]\n",
    "            \n",
    "            state_next, reward, done, _ = env.step(action_choice)\n",
    "            state = state_next\n",
    "            \n",
    "            episode_rewards += reward\n",
    "            \n",
    "            if done or step + 1 == max_steps_per_episode:\n",
    "                print(\"Rewards for episode \" + str(episode) + \": \" + str(episode_rewards))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
